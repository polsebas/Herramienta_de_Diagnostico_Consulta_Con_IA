# Configuración del Sistema de Evaluación - Golden Set y Métricas de Calidad

# Configuración general del evaluador
general:
  evaluation_mode: "comprehensive"  # comprehensive, quick, focused
  enable_auto_evaluation: true
  evaluation_schedule: "daily"      # daily, weekly, monthly, manual
  evaluation_timeout_minutes: 30
  max_parallel_evaluations: 3
  auto_retry_failed: true
  max_retry_attempts: 3

# Umbrales de calidad
evaluation_thresholds:
  excellent: 0.9    # 90-100%
  good: 0.8         # 80-89%
  acceptable: 0.7   # 70-79%
  poor: 0.6         # 60-69%
  # < 60% = unacceptable

# Pesos de las métricas de calidad
quality_weights:
  accuracy: 0.25        # Precisión del tipo de tarea detectado
  completeness: 0.20    # Completitud de componentes esperados
  relevance: 0.15       # Relevancia del contrato generado
  actionability: 0.15   # Capacidad de acción del plan
  security: 0.10        # Consideraciones de seguridad
  performance: 0.05     # Consideraciones de performance
  maintainability: 0.05 # Mantenibilidad del código
  testability: 0.03     # Capacidad de testing
  documentation: 0.02   # Calidad de documentación

# Configuración del Golden Set
golden_set:
  # Preguntas de Autenticación y Autorización
  auth:
    enabled: true
    questions_count: 2
    baseline_score_target: 0.85
    difficulty_distribution:
      easy: 0.2
      medium: 0.5
      hard: 0.3
  
  # Preguntas de Seguridad
  security:
    enabled: true
    questions_count: 2
    baseline_score_target: 0.80
    require_human_approval: true
    critical_risk_threshold: 0.75
  
  # Preguntas de Performance
  performance:
    enabled: true
    questions_count: 2
    baseline_score_target: 0.83
    performance_metrics:
      - "execution_time"
      - "memory_usage"
      - "throughput"
  
  # Preguntas de Testing
  testing:
    enabled: true
    questions_count: 2
    baseline_score_target: 0.87
    coverage_threshold: 0.80
    test_types:
      - "unit"
      - "integration"
      - "performance"
      - "security"
  
  # Preguntas de Documentación
  documentation:
    enabled: true
    questions_count: 2
    baseline_score_target: 0.90
    doc_quality_metrics:
      - "completeness"
      - "clarity"
      - "examples"
      - "structure"
  
  # Preguntas de DevOps
  devops:
    enabled: true
    questions_count: 2
    baseline_score_target: 0.78
    ci_cd_metrics:
      - "build_success_rate"
      - "deployment_speed"
      - "rollback_capability"
  
  # Preguntas de Refactoring
  refactor:
    enabled: true
    questions_count: 2
    baseline_score_target: 0.80
    refactor_metrics:
      - "code_complexity_reduction"
      - "maintainability_improvement"
      - "test_coverage_maintained"
  
  # Preguntas de Análisis
  analysis:
    enabled: true
    questions_count: 2
    baseline_score_target: 0.85
    analysis_depth_metrics:
      - "root_cause_identification"
      - "solution_effectiveness"
      - "impact_assessment"
  
  # Preguntas de Migración
  migration:
    enabled: true
    questions_count: 2
    baseline_score_target: 0.74
    migration_safety_metrics:
      - "backup_strategy"
      - "rollback_plan"
      - "data_integrity"
  
  # Preguntas de Monitoreo
  monitoring:
    enabled: true
    questions_count: 2
    baseline_score_target: 0.81
    monitoring_metrics:
      - "alert_accuracy"
      - "response_time"
      - "coverage"

# Configuración de métricas de evaluación
metrics:
  # Métricas de Contrato
  contract:
    type_detection_accuracy: true
    risk_assessment_accuracy: true
    component_completeness: true
    human_approval_requirement: true
  
  # Métricas de Ejecución
  execution:
    task_success_rate: true
    execution_time: true
    resource_usage: true
    error_rate: true
  
  # Métricas de Artefactos
  artifacts:
    file_generation_success: true
    code_quality: true
    test_coverage: true
    documentation_quality: true
  
  # Métricas de Seguridad
  security:
    vulnerability_detection: true
    compliance_check: true
    risk_mitigation: true
    security_score: true

# Configuración de exportación de resultados
export:
  enabled: true
  formats:
    - "json"
    - "csv"
    - "excel"
    - "html"
  
  # Generación de gráficos
  charts:
    enabled: true
    chart_types:
      - "quality_distribution"
      - "domain_performance"
      - "difficulty_performance"
      - "trend_analysis"
      - "comparison_charts"
    
    # Configuración de gráficos
    chart_config:
      dpi: 300
      format: "png"
      style: "seaborn-v0_8"
      figure_size: [12, 8]
  
  # Directorio de salida
  output_directory: "eval/results"
  filename_template: "evaluation_{timestamp}_{type}.{extension}"

# Configuración de notificaciones
notifications:
  enabled: true
  channels:
    - "console"
    - "file"
    - "email"
    - "slack"
    - "webhook"
  
  # Eventos que disparan notificaciones
  events:
    evaluation_started: true
    evaluation_completed: true
    quality_threshold_breach: true
    improvement_detected: true
    critical_issues_found: true
  
  # Umbrales para notificaciones
  thresholds:
    quality_alert: 0.7      # Alertar si score < 70%
    critical_alert: 0.6     # Alertar si score < 60%
    improvement_threshold: 0.1  # Notificar mejoras > 10%

# Configuración de integración
integration:
  # Integración con Context Manager
  context_manager:
    enabled: true
    context_relevance_threshold: 0.7
    max_context_chunks: 10
  
  # Integración con Human-in-the-Loop
  human_loop:
    enabled: true
    auto_approval_threshold: 0.8
    manual_review_threshold: 0.6
    escalation_threshold: 0.5
  
  # Integración con Cursor Agent
  cursor_agent:
    enabled: true
    task_execution_timeout: 300  # 5 minutos
    max_concurrent_tasks: 2
    workspace_validation: true

# Configuración de desarrollo y testing
development:
  debug_mode: false
  verbose_logging: false
  mock_evaluations: false
  test_mode: false
  
  # Configuración de testing
  testing:
    enable_mock_agents: false
    mock_response_time: 2.0  # segundos
    mock_success_rate: 0.9
    mock_quality_scores:
      accuracy: 0.85
      completeness: 0.80
      relevance: 0.90
      actionability: 0.85

# Configuración de auditoría
audit:
  enabled: true
  log_all_evaluations: true
  audit_file: "logs/audit.jsonl"
  retention_days: 90
  
  # Campos a auditar
  audit_fields:
    - "question_id"
    - "timestamp"
    - "agent_version"
    - "contract_generated"
    - "task_executed"
    - "quality_scores"
    - "overall_score"
    - "feedback"
    - "metadata"

# Configuración de performance
performance:
  # Límites de recursos
  resource_limits:
    max_memory_mb: 1024
    max_cpu_percent: 80
    max_disk_usage_mb: 100
  
  # Optimizaciones
  optimizations:
    parallel_processing: true
    batch_processing: true
    caching: true
    lazy_evaluation: false
  
  # Timeouts
  timeouts:
    question_evaluation: 300      # 5 minutos por pregunta
    contract_generation: 60       # 1 minuto por contrato
    task_execution: 600           # 10 minutos por tarea
    overall_evaluation: 3600      # 1 hora total

# Configuración de reporting
reporting:
  enabled: true
  report_types:
    - "summary"
    - "detailed"
    - "trend_analysis"
    - "recommendations"
    - "comparison"
  
  # Frecuencia de reportes
  frequency:
    summary: "daily"
    detailed: "weekly"
    trend_analysis: "monthly"
    recommendations: "weekly"
  
  # Destinatarios de reportes
  recipients:
    developers: true
    product_managers: true
    security_team: true
    operations_team: true
  
  # Formato de reportes
  format:
    default: "html"
    alternatives: ["pdf", "markdown", "json"]
    include_charts: true
    include_recommendations: true
